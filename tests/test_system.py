import cv2
import pytest
import subprocess
import os
import time
from config import *
from utils import *

import shutil
import socket

def is_running_in_docker():
    """
    Detect if the tests are running inside a Docker container by checking for the presence of /.dockerenv file.
    """
    return os.path.exists('/.dockerenv')

@pytest.mark.skipif(not is_running_in_docker(), reason="Test is designed to run inside Docker container")
def test_docker_container_setup():
    """
    This test validates the Docker container environment to ensure all necessary components are in place for the system to function correctly.
        - Checks that FFmpeg is installed and accessible in the container.
        - Verifies that the AI model XML file is present in the expected location.
        - Confirms that the video file required for streaming exists.
        - Validates basic networking capabilities of the container to ensure it can communicate with localhost.
 """
    print("\n[+] Running Docker Environment Validation...")
    assert shutil.which("ffmpeg") is not None, "FFmpeg is not installed or not in PATH within the Docker container."
    assert os.path.exists("cars.xml"), "AI model file 'cars.xml' is missing in the Docker container."
    assert os.path.exists("videoRoadTraffic.mp4"), "Video file 'videoRoadTraffic.mp4' is missing in the Docker container."
    try:
        host_name = socket.gethostname()
        host_ip = socket.gethostbyname(host_name)
        assert host_ip is not None
        print(f"[+] Container Networking: OK (IP: {host_ip})")
    except Exception as e:
        pytest.fail(f"Failed to validate container networking: {e}")
    print("[+] Docker Setup Validation: PASSED")


@pytest.fixture(scope="module")
def start_system():
    """
    Fixture to start sender and receiver processes before tests and clean up after.
    """
    remove_file_if_exists([SDP_FILE,METRICS_FILE])
    processes = []
    try:
        sender_proc = subprocess.Popen(COMMANDS["sender"], **SUBPROCESS_CONFIG)
        processes.append(sender_proc)
        if sender_proc.poll() is None:
            print("[+]Sender process is running.")
        wait_for_file(
            file_path=SDP_FILE,
            timeout=10,
            cleanup_proc=sender_proc,
            error_msg="[!] Sender failed to start stream",
        )
        receiver_proc = subprocess.Popen(COMMANDS["receiver"], **SUBPROCESS_CONFIG)
        processes.append(receiver_proc)
        if receiver_proc.poll() is None:
            print("[+] Receiver process is running.")
        yield {"sender": sender_proc, "receiver": receiver_proc}   
    finally:
        print("\n---Final Cleanup: Killing all processes---")
        for proc in processes:
            try:
                if proc.poll() is None:
                    if os.name == 'nt':  # Windows
                        kill_process_tree(proc.pid)
                    else:  # Linux / Docker
                        # שליחת סיגנל סגירה לקבוצת התהליכים (Process Group)
                        os.killpg(os.getpgid(proc.pid), signal.SIGTERM)
            except Exception as e:
                print(f"[-] Debug: Individual process cleanup failed: {e}")
                proc.terminate()
        try:
            cv2.destroyAllWindows()
        except:
            pass
        if os.name == 'nt':
            os.system(TASK_KILL_FFMPEG)
            os.system(TASK_KILL_PYTHON)
        else:
            os.system("pkill -9 -f ffmpeg || true")
            os.system("pkill -9 -f receiver.py || true")
# --- AUTOMATION TESTS---

def test_sdp_generation(start_system):
    """
    SDP Validation Test:
    """
    assert os.path.exists(
        SDP_FILE
    ), f"Critical Error: {SDP_FILE} was not generated by the sender!"
    file_size = os.path.getsize(SDP_FILE)
    assert file_size > 0, "SDP file exists but it is empty!"
    with open(SDP_FILE, "r") as f:
        content = f.read()
        assert (
            "m=video 5004" in content
        ), "SDP file content is invalid -missing port 5004"


def test_processes_are_running(start_system):
    """
    Verify that both sender and receiver processes are alive.
    """
    assert start_system["sender"].poll() is None, "Sender process died!"
    assert start_system["receiver"].poll() is None, "Receiver process died!"


def test_metrics_collection_flow(start_system):
    """
    Verify that the receiver is actively processing frames and updating metrics.json with expected fields.
    """
    metrics_data = wait_for_metrics_content(METRICS_FILE)
    assert (
        metrics_data is not None
    ), f"Failed to collect 5 frames within timeout in {METRICS_FILE}"
    expected_keys = [
        "total_frames",
        "detected_frames",
        "detection_ratio",
        "avg_latency_ms",
        "max_latency_ms",
    ]
    for key in expected_keys:
        assert key in metrics_data, f"Expected key '{key}' not found in metrics.json"


def test_metrics_data_integrity(start_system):
    """
    Verify that the calculated metrics in metrics.json are logically consistent and within expected bounds.
    """
    data = wait_for_metrics_content(METRICS_FILE)
    total = data["total_frames"]
    detected = data["detected_frames"]
    ratio = data["detection_ratio"]
    avg_latency = data["avg_latency_ms"]
    max_latency = data["max_latency_ms"]
    assert total > 0, "AI Logic Error: Model failed to process any frames in a video that contains traffic"
    assert detected > 0, "AI Logic Error: Model failed to detect any cars in a video that contains traffic"
    assert 0 <= ratio <= 1.0, f"Detection ratio out of bounds: {ratio}"   
    expected_ratio = round(detected / total, 2)
    assert ratio == expected_ratio,f"Detection ratio mismatch! Expected {expected_ratio}, got {ratio}"
    assert avg_latency >= 0, "Latency cannot be negative"
    assert max_latency >= avg_latency, "Logic Error: Max latency cannot be smaller than average"

def test_metrics_collection_is_alive(start_system):
    """
    Verify system is actively processing frames and updating metrics
    """
    is_growing, final_data = wait_for_data_growth(METRICS_FILE)
    assert final_data is not None, "System failed to collect any initial data"
    assert (
        is_growing
    ), f"Data is stagnant! Frame count stayed at {final_data.get('total_frames')}"
    assert final_data["avg_latency_ms"] > 0, "Latency metrics are missing or zero"
    assert "detected_frames" in final_data, "Detection field missing in JSON"
    print(
        f"[+] Verified: System is alive. Frames processed: {final_data['total_frames']}"
    )


def test_data_stability(start_system):
    """
    Verify that the system maintains stable performance metrics over time without significant degradation or spikes.
     - Check that detection ratio remains above a reasonable threshold (0.3) to ensure model is performing adequately.
     - Check that average latency remains below a certain threshold (500ms) to ensure system is responsive.
     - Check that max latency does not have extreme spikes (above 15 seconds) which could indicate instability or resource issues.
    """  
    data = wait_for_metrics_content(METRICS_FILE)
    avg = data["avg_latency_ms"]
    maximum = data["max_latency_ms"]
    ratio = data["detection_ratio"]
    assert ratio > 0.3, f"AI Weakness: Detection ratio too low ({ratio}). Model might be struggling."
    assert maximum < 15000, f"Latency spike is too high: {maximum}ms"
    assert avg < 500, f"System is too slow! Average latency: {avg}ms"


def test_system_crash_and_recovery(start_system):
    """
    This test simulates a crash of the sender process and verifies that:
    1. The receiver continues to run and does not crash.
    2. The metrics.json file is not lost and retains the data collected before the crash.
    3. After restarting the sender, the system recovers and continues to collect data, showing growth in total_frames.
    """
    initial_data = wait_for_metrics_content(METRICS_FILE, min_frames=5)
    frames_before = initial_data["total_frames"]  
    print("\n---Simulating sender crash...---")
    kill_process_tree(start_system["sender"].pid)
    time.sleep(2)
    assert os.path.exists(METRICS_FILE), "Metrics file lost after crash"  
    print("---Restarting sender for recovery...---")
    new_sender = subprocess.Popen(COMMANDS["sender"], **SUBPROCESS_CONFIG)
    if new_sender.poll() is None:
        print("[+] New sender process started successfully.")       
    is_growing, final_data = wait_for_data_growth(METRICS_FILE)
    frames_after = final_data["total_frames"] if final_data else 0
    assert is_growing, "System failed to recover and resume data collection"
    assert frames_after > frames_before, "Metrics did not accumulate after recovery"
    print(f"[+] System fully recovered. Total frames: {frames_after}")


