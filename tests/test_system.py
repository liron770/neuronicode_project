import pytest
import subprocess
import os
import json
from config import *
from utils import *


@pytest.fixture(scope="module")
def start_system():
    """
    Fixture to start sender and receiver processes before tests and clean up after.
    """
    os.system(TASK_KILL_FFMPEG)
    for f in [SDP_FILE, METRICS_FILE]:
        if os.path.exists(f):
            os.remove(f)

    sender_proc = subprocess.Popen(COMMANDS["sender"], **SUBPROCESS_CONFIG)

    if sender_proc.poll() is None:
        print("Sender process is running.")

    wait_for_file(
        file_path=SDP_FILE,
        timeout=10,
        cleanup_proc=sender_proc,
        error_msg="Sender failed to start stream",
    )

    receiver_proc = subprocess.Popen(COMMANDS["receiver"], **SUBPROCESS_CONFIG)

    if receiver_proc.poll() is None:
        print("Receiver process is running.")

    yield {"sender": sender_proc, "receiver": receiver_proc}
    processes = [receiver_proc, sender_proc]
    cleanup_processes(processes)


# --- AUTOMATION TESTS---


def test_sdp_generation():
    """
    SDP Validation Test:
    """
    assert os.path.exists(
        SDP_FILE
    ), f"Critical Error: {SDP_FILE} was not generated by the sender!"

    file_size = os.path.getsize(SDP_FILE)
    assert file_size > 0, "SDP file exists but it is empty!"

    with open(SDP_FILE, "r") as f:
        content = f.read()
        assert (
            "m=video 5004" in content
        ), "SDP file content is invalid (missing port 5004)"


def test_processes_are_running(start_system):
    """
    Verify that both sender and receiver processes are alive.
    """
    assert start_system["sender"].poll() is None, "Sender process died!"
    assert start_system["receiver"].poll() is None, "Receiver process died!"


def test_metrics_collection_flow():
    metrics_data = wait_for_metrics_content(METRICS_FILE, min_frames=5)
    assert (
        metrics_data is not None
    ), f"Failed to collect 5 frames within timeout in {METRICS_FILE}"
    expected_keys = [
        "total_frames",
        "detected_frames",
        "detection_ratio",
        "avg_latency_ms",
        "max_latency_ms",
    ]
    for key in expected_keys:
        assert key in metrics_data, f"Expected key '{key}' not found in metrics.json"
    print(f"Verified: Collected {metrics_data['total_frames']} frames.")


def test_metrics_collection_is_alive():
    """
    Verify system is actively processing frames and updating metrics
    """
    is_growing, final_data = wait_for_data_growth(METRICS_FILE)
    assert final_data is not None, "System failed to collect any initial data"
    assert (
        is_growing
    ), f"Data is stagnant! Frame count stayed at {final_data.get('total_frames')}"
    assert final_data["avg_latency_ms"] > 0, "Latency metrics are missing or zero"
    assert "detected_frames" in final_data, "Detection field missing in JSON"

    print(
        f"[+] Verified: System is alive. Frames processed: {final_data['total_frames']}"
    )


def test_latency_logic():
    with open(METRICS_FILE, "r") as f:
        data = json.load(f)
    assert (
        data["max_latency_ms"] >= data["avg_latency_ms"]
    ), "Logic Error: Max latency cannot be smaller than average"
